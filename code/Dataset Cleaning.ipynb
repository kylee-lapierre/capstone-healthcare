{"cells":[{"cell_type":"code","source":["import requests\nimport pyspark.sql.functions\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import StringType,DecimalType\nfrom pyspark.sql.functions import input_file_name, substring\nfrom pyspark.sql.functions import isnan, when, count, col\nfrom sodapy import Socrata"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb877102-d79f-4a8b-a5f4-faaeb1285a96"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [\"testdemo\"], \"version\": \"0.0.1\"}"},"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [\"testdemo\"], \"version\": \"0.0.1\"}"}}],"execution_count":0},{"cell_type":"code","source":["###### Mount Point 1 through Oauth security.\nstorageAccount = \"gen10datafund2207\"\nstorageContainer = \"healthcare-capstone-group3\"\nclientSecret = \"Cty8Q~AvEO_qC-MjvPvosYauiNsffOHKnMpj7cmd\"\nclientid = \"2ca50102-5717-4373-b796-39d06568588d\"\nmount_point = \"/mnt/healthcare/dataIn\" # the mount point will be unique to you\n\nconfigs = {\"fs.azure.account.auth.type\": \"OAuth\",\n       \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n       \"fs.azure.account.oauth2.client.id\": clientid,\n       \"fs.azure.account.oauth2.client.secret\": clientSecret,\n       \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/d46b54b2-a652-420b-aa5a-2ef7f8fc706e/oauth2/token\",\n       \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n\ntry: \n    dbutils.fs.unmount(mount_point)\nexcept:\n    pass\n\ndbutils.fs.mount(\nsource = \"abfss://\"+storageContainer+\"@\"+storageAccount+\".dfs.core.windows.net/\",\nmount_point = mount_point,\nextra_configs = configs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4341891-4a4b-4bea-8183-e24c9f0e9db3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/mnt/healthcare/dataIn has been unmounted.\nOut[24]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/mnt/healthcare/dataIn has been unmounted.\nOut[24]: True"]}}],"execution_count":0},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/mnt/healthcare/dataIn/ModelData\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"379ef4da-2ef8-4041-9eb4-9295a0884c1a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/mnt/healthcare/dataIn/ModelData/Demographics Test Data 2020.csv","Demographics Test Data 2020.csv",3303579,1664465328000],["dbfs:/mnt/healthcare/dataIn/ModelData/Health Insurance Characteristics 2015.csv","Health Insurance Characteristics 2015.csv",1484769,1664223416000],["dbfs:/mnt/healthcare/dataIn/ModelData/Health Insurance Characteristics 2016.csv","Health Insurance Characteristics 2016.csv",1484129,1664223416000],["dbfs:/mnt/healthcare/dataIn/ModelData/Health Insurance Characteristics 2017.csv","Health Insurance Characteristics 2017.csv",1473853,1664223416000],["dbfs:/mnt/healthcare/dataIn/ModelData/Health Insurance Characteristics 2018.csv","Health Insurance Characteristics 2018.csv",1476522,1664223416000],["dbfs:/mnt/healthcare/dataIn/ModelData/Health Insurance Characteristics.csv","Health Insurance Characteristics.csv",1480572,1664374667000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/healthcare/dataIn/ModelData/Demographics Test Data 2020.csv</td><td>Demographics Test Data 2020.csv</td><td>3303579</td><td>1664465328000</td></tr><tr><td>dbfs:/mnt/healthcare/dataIn/ModelData/Health Insurance Characteristics 2015.csv</td><td>Health Insurance Characteristics 2015.csv</td><td>1484769</td><td>1664223416000</td></tr><tr><td>dbfs:/mnt/healthcare/dataIn/ModelData/Health Insurance Characteristics 2016.csv</td><td>Health Insurance Characteristics 2016.csv</td><td>1484129</td><td>1664223416000</td></tr><tr><td>dbfs:/mnt/healthcare/dataIn/ModelData/Health Insurance Characteristics 2017.csv</td><td>Health Insurance Characteristics 2017.csv</td><td>1473853</td><td>1664223416000</td></tr><tr><td>dbfs:/mnt/healthcare/dataIn/ModelData/Health Insurance Characteristics 2018.csv</td><td>Health Insurance Characteristics 2018.csv</td><td>1476522</td><td>1664223416000</td></tr><tr><td>dbfs:/mnt/healthcare/dataIn/ModelData/Health Insurance Characteristics.csv</td><td>Health Insurance Characteristics.csv</td><td>1480572</td><td>1664374667000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Cleaning the Test Demo Data**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b2c922f-dc75-4502-ac4b-701225043b19"}}},{"cell_type":"code","source":["testdemo = spark.read.options(header = 'True').csv(\"/mnt/healthcare/dataIn/ModelData/Demographics Test Data 2020.csv\").toPandas()\n\n# Counting the NA values in each column\ntestdemo.isna().sum()\n\n# Dropping the NA rows since there was 11 in every row\ntestdemo.dropna(inplace=True)\n\n# Making sure all NA values are gone\ntestdemo.isna().sum()\n\nCategory = []\n\nfor i in testdemo['Label (Grouping)']:\n    Category.append(i.strip())\n\ntestdemo['Label (Grouping)'] = Category\n\nnewcolumns = list(testdemo['Label (Grouping)'])\n\ntestdemo.drop(columns = ['United States!!Insured!!Estimate', 'United States!!Uninsured!!Estimate'], inplace = True)\ntestdemo\n\n# Transposing so the states become the rows and the categories become the columns\ntestdemo = testdemo.transpose()\n\ncategories = list(testdemo.index)[1:]\nCounties = ['County']\nStates = ['State']\nInsurance = ['PopCategory']\ntestdemo\nfor i in categories:\n    category = i.removesuffix('!!Estimate')\n    state = (category.split(',')[1])\n    Counties.append(category.split(',')[0])\n    States.append(state.split('!!')[0])\n    Insurance.append(state.split('!!')[1])\n\ntestdemo[72] = Counties\ntestdemo[73] = States\ntestdemo[74] = Insurance\n\nnewcolumns.append('County')\nnewcolumns.append('State')\nnewcolumns.append('PopCategory')\n\ntestdemo.columns = newcolumns\n\ntestdemo.drop(index = 'Label (Grouping)', inplace = True)\n\nindeces = []\n\nfor i in range(0, testdemo.shape[0]):\n    indeces.append(i)\ntestdemo[''] = indeces\ntestdemo.set_index('', inplace = True)\n\n\ntestdemo.drop(columns = ['Total household population', 'In non-family households and other living arrangements', \\\n    'In family households', 'In married couple families', 'In other families', 'Male reference person, no spouse present', \\\n    'Female reference person, no spouse present', 'With a disability', 'No disability', 'Civilian noninstitutionalized population 19 to 64 years', \\\n    'In labor force', 'Employed', 'Unemployed', 'Not in labor force', 'Worked full-time, year round in the past 12 months', \\\n    'Worked less than full-time, year round in the past 12 months', 'Did not work', 'Civilian noninstitutionalized population for whom poverty status is determined', \\\n    'Civilian noninstitutionalized population 26 years and over', 'Under 19 years', '19 to 64 years', '65 years and older', 'Two or more races', \\\n    'White alone, not Hispanic or Latino', 'Below 138 percent of the poverty threshold', '138 to 399 percent of the poverty threshold', \\\n    'At or above 400 percent of the poverty threshold', 'Below 100 percent of the poverty threshold'], inplace = True)\n\ncolumns = list(testdemo.columns)[:-3]\nfor i in columns:\n       testdemo = testdemo.replace([','],'', regex = True)\n       testdemo[i] = pd.to_numeric(testdemo[i])\n\n\ntestdemo[columns] = testdemo[columns].astype(int)\n\nnewColumns = ['State', 'County', 'PopCategory', \\\n    'Civilian noninstitutionalized population', 'Under 6 years', '6 to 18 years', '19 to 25 years', '26 to 34 years', '35 to 44 years', \\\n    '45 to 54 years', '55 to 64 years', '65 to 74 years', '75 years and older', 'Male', \\\n    'Female', 'White alone', 'Black or African American alone', 'American Indian and Alaska Native alone', 'Asian alone', 'Native Hawaiian and Other Pacific Islander alone', \\\n    'Some other race alone', 'Hispanic or Latino (of any race)', \\\n    'Native born', 'Foreign born', 'Naturalized', 'Not a citizen', \\\n    'Less than high school graduate', 'High school graduate (includes equivalency)', \\\n    \"Some college or associate's degree\", \"Bachelor's degree or higher\", 'Under $25,000', '$25,000 to $49,999', \\\n    '$50,000 to $74,999', '$75,000 to $99,999', '$100,000 and over']\n\ntestdemo = testdemo[newColumns]\n\ntestdemo.columns = ['State', 'County', 'Insurance_Category', \\\n    'Total_Population', 'Under_6Y', '_6_to_18Y', '_19_to_25Y', '_26_to_34Y', '_35_to_44Y', \\\n    '_45_to_54Y', '_55_to_64Y', '_65_to_74Y', '_75_and_Older', 'Male', \\\n    'Female', 'White', 'African_American', 'American_Indian', 'Asian', 'Pacific_Islander', \\\n    'Some_Other_Race', 'Hispanic', \\\n    'Native_Born', 'Foreign_Born', 'Naturalized', 'Not_A_Citizen', \\\n    'Less_Than_High_school', 'High_School_or_Equivalent', \\\n    \"Some_College\", \"Bachelors_or_Higher\", 'Under_25000S', '_25000_to_49999S', \\\n    '_50000_to_74999S', '_75000_to_99999S', 'Over_100000S']\n\ntestdemo['State'] = testdemo['State'].str.lstrip()\ntestdemo['State'] = testdemo['State'].str.replace(' ', '_')\ntestdemo['State'] = testdemo['State'].replace('District_of_Columbia', 'DC')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d3ba31c-d4ee-4d35-b81f-eec407643c79"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Cleaning the Demographic dataset**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3432dace-7aa6-46aa-8a0e-e187552ac399"}}},{"cell_type":"code","source":["demographics = spark.read.options(header = 'True').csv('/mnt/healthcare/dataIn/Demographics and Housing Estimates.csv')\ndemographics = demographics.toPandas()\n\ndemographics.dropna(inplace = True)\n\ncolumns = demographics.columns\n\nStates = []\nnewStates = []\n\nfor i in columns:\n    States.append(i.removesuffix(\"!!Estimate\"))\n\nfor i in States:\n    newStates.append(i.replace(' ', '_'))\n\n# Rows to drop:\nrows = [4, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 73, 75, 76, 77, 78, 79, 86, 87, 88, 89, 91, 92, 93]\ndemographics.drop(index = rows, inplace = True)\n\ndemoRename = demographics.transpose()\n\ndemoRename[94] = newStates\n\ndemoRename.drop(index = 'Label (Grouping)', inplace = True)\n\nindeces = []\nfor i in range(0, demoRename.shape[0]):\n    indeces.append(i)\ndemoRename[''] = indeces\ndemoRename.set_index('', inplace = True)\n\ndemoRename.columns = ['Total_Population', 'Male', 'Female', 'Under_5Y', '_5_to_9Y', '_10_to_14Y', '_15_to_19Y', '_20_to_24Y', \\\n    '_25_to_34Y', '_35_to_44Y', '_45_to_54Y', '_55_to_59Y', '_60_to_64Y', '_65_to_74Y', '_75_to_84Y', '_85_and_Older', \\\n    'Hispanic', 'White', 'African_American', 'American_Indian', 'Asian', \\\n    'Pacific_Islander', 'Some_Other_Race', 'State']\n\ndemoRename = demoRename[['State', 'Total_Population', 'Male', 'Female', 'Under_5Y', '_5_to_9Y', '_10_to_14Y', '_15_to_19Y', '_20_to_24Y', \\\n    '_25_to_34Y', '_35_to_44Y', '_45_to_54Y', '_55_to_59Y', '_60_to_64Y', '_65_to_74Y', '_75_to_84Y', '_85_and_Older', \\\n    'Hispanic', 'White', 'African_American', 'American_Indian', 'Asian', \\\n    'Pacific_Islander', 'Some_Other_Race']]\n\nreference = list(demoRename.columns)[1:]\nfor i in reference:\n    demoRename[i] = demoRename[i].str.replace(',', '')\n    demoRename[i] = pd.to_numeric(demoRename[i])\n\ndemoRename['State'].replace('District_of_Columbia', 'DC', inplace = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a242a323-f353-489e-a074-23de2734c3bd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Cleaning the Hospital Counts dataset**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7b1a3de-f126-48d8-9dea-c4699415561d"}}},{"cell_type":"code","source":["hospitals = spark.read.options(header = 'True').csv('/mnt/healthcare/dataIn/Hospital Count.csv')\nhospitals = hospitals.toPandas()\n\n# Renaming the columns to make them easier to reference\nhospitals.rename(columns = {'Geographic Area Name (NAME)':'States', \\\n                            '2017 NAICS code (NAICS2017)':'StateCode', \\\n                            'Meaning of NAICS code (NAICS2017_LABEL)':'Label', \\\n                            'Meaning of Legal form of organization code (LFO_LABEL)':'Different_Establishments', \\\n                            'Meaning of Employment size of establishments code (EMPSZES_LABEL)':'Establishment_Size', \\\n                            'Year (YEAR)':'Year', \\\n                            'Number of establishments (ESTAB)':'Total_Establishments', \\\n                            'Annual payroll ($1,000) (PAYANN)':'Annual_Payroll', \\\n                            'First-quarter payroll ($1,000) (PAYQTR1)':'Q1Payroll', \\\n                            'Number of employees (EMP)':'Employee_Size'}, inplace = True)\n\n# Creating a new dataframe with the columns we are interested in\nnewhospitals = hospitals[['States', 'Different_Establishments', 'Establishment_Size', 'Total_Establishments', 'Employee_Size']].copy()\n\n# Changing the type of the Employee_Size column to an integer\nnewhospitals['Employee_Size'] = newhospitals['Employee_Size'].str.replace(',', '')\nnewhospitals['Employee_Size'] = pd.to_numeric(newhospitals['Employee_Size'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c966e3b0-3d48-4429-8acc-84bbb9d833c7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Cleaning the Health Insurance Characteristics dataset**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06356955-87b3-47b6-8d9f-6ebb060df7c1"}}},{"cell_type":"code","source":["health = spark.read.options(header = 'True').csv('/mnt/healthcare/dataIn/Health Insurance Characteristics.csv')\nhealth = health.toPandas()\n\n# Counting the NA values in each column\nhealth.isna().sum()\n\n# Dropping the NA rows since there was 11 in every row\nhealth.dropna(inplace=True)\n\n# Making sure all NA values are gone\nhealth.isna().sum()\n\nCategory = []\n\nfor i in health['Label (Grouping)']:\n    Category.append(i.strip())\n\nhealth['Label (Grouping)'] = Category\n\nnewcolumns = list(health['Label (Grouping)'])\n\n# Transposing so the states become the rows and the categories become the columns\nhealth = health.transpose()\n\ncategories = list(health.index)[1:]\nCounties = ['County']\nStates = ['State']\nInsurance = ['PopCategory']\n\nfor i in categories:\n    state = i.split(',')[1]\n    Counties.append(i.split(',')[0])\n    States.append(state.split('!!')[0])\n    Insurance.append(state.split('!!')[1])\n\nhealth['61'] = Counties\nhealth['62'] = States\nhealth['63'] = Insurance\n\nnewcolumns.append('County')\nnewcolumns.append('State')\nnewcolumns.append('PopCategory')\n\nhealth.columns = newcolumns\n\nhealth.drop(index = 'Label (Grouping)', inplace = True)\n\nindeces = []\n\nfor i in range(0, health.shape[0]):\n    indeces.append(i)\nhealth[''] = indeces\nhealth.set_index('', inplace = True)\n\nhealth.replace('N', 0, inplace = True)\n\nhealth.drop(columns = ['Total household population', 'In non-family households and other living arrangements', \\\n    'In family households', 'In married couple families', 'In other families', 'Male reference person, no spouse present', \\\n    'Female reference person, no spouse present', 'With a disability', 'No disability', 'Civilian noninstitutionalized population 19 to 64 years', \\\n    'In labor force', 'Employed', 'Unemployed', 'Not in labor force', 'Worked full-time, year round in the past 12 months', \\\n    'Worked less than full-time, year round in the past 12 months', 'Did not work', 'Civilian noninstitutionalized population for whom poverty status is determined', \\\n    'Civilian noninstitutionalized population 26 years and over', 'Under 19 years', '19 to 64 years', '65 years and older', 'Two or more races', \\\n    'White alone, not Hispanic or Latino', 'Below 138 percent of the poverty threshold', '138 to 399 percent of the poverty threshold', \\\n    'At or above 400 percent of the poverty threshold', 'Below 100 percent of the poverty threshold'], inplace = True)\n\ncolumns = list(health.columns)[:-3]\nfor i in columns:\n       health[i] = health[i].str.replace(',', '')\n       health[i] = pd.to_numeric(health[i])\n\n# Imputing the 0 values for the mean of the population\nhealth.fillna(0, inplace = True)\nimp = SimpleImputer(strategy = 'mean', missing_values = 0)\n\nhealth[columns] = imp.fit_transform(health[columns])\nhealth[columns] = health[columns].astype(int)\n\nnewColumns = ['State', 'County', 'PopCategory', \\\n    'Civilian noninstitutionalized population', 'Under 6 years', '6 to 18 years', '19 to 25 years', '26 to 34 years', '35 to 44 years', \\\n    '45 to 54 years', '55 to 64 years', '65 to 74 years', '75 years and older', 'Male', \\\n    'Female', 'White alone', 'Black or African American alone', 'American Indian and Alaska Native alone', 'Asian alone', 'Native Hawaiian and Other Pacific Islander alone', \\\n    'Some other race alone', 'Hispanic or Latino (of any race)', \\\n    'Native born', 'Foreign born', 'Naturalized', 'Not a citizen', \\\n    'Less than high school graduate', 'High school graduate (includes equivalency)', \\\n    \"Some college or associate's degree\", \"Bachelor's degree or higher\", 'Under $25,000', '$25,000 to $49,999', \\\n    '$50,000 to $74,999', '$75,000 to $99,999', '$100,000 and over']\n\nhealth = health[newColumns]\n\nhealth.columns = ['State', 'County', 'Insurance_Category', \\\n    'Total_Population', 'Under_6Y', '_6_to_18Y', '_19_to_25Y', '_26_to_34Y', '_35_to_44Y', \\\n    '_45_to_54Y', '_55_to_64Y', '_65_to_74Y', '_75_and_Older', 'Male', \\\n    'Female', 'White', 'African_American', 'American_Indian', 'Asian', 'Pacific_Islander', \\\n    'Some_Other_Race', 'Hispanic', \\\n    'Native_Born', 'Foreign_Born', 'Naturalized', 'Not_A_Citizen', \\\n    'Less_Than_High_school', 'High_School_or_Equivalent', \\\n    \"Some_College\", \"Bachelors_or_Higher\", 'Under_25000S', '_25000_to_49999S', \\\n    '_50000_to_74999S', '_75000_to_99999S', 'Over100000S']\n\nhealth['State'] = health['State'].str.lstrip()\nhealth['State'] = health['State'].str.replace(' ', '_')\nhealth['State'] = health['State'].replace('District_of_Columbia', 'DC')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7582654-0ef3-42bb-83ef-5176587245eb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Cleaning NHIS dataset**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"064086b7-048a-4c80-9538-d9f477fe57ed"}}},{"cell_type":"code","source":["nhis = spark.read.options(header = 'True').csv('/mnt/healthcare/dataIn/adult19.csv')\n\nnhis = nhis.toPandas()\n\n# selecting the columns to keep: includes the variables that focus on health insurance, utilization of healthcare,\n# and demographics including marital status, citizenship status, veteran status, sexual orientation, sex, age,\n# ethnicity, income, and general region of the country (one of four regions)\n\n# see other documents for specific columns\n\ncolumns = [c for c in nhis.columns if c.startswith('PAYBLL')\n                                    or c.startswith('PAYWORRY')\n                                    or c.startswith('MHTHD')\n                                    or c.startswith('ORIE')\n                                    or c.startswith('MARITAL')\n                                    or c.startswith('AGEP_A')\n                                    or c.startswith('SEX')\n                                    or c.startswith('ED')\n                                    or c.startswith('NATUS')\n                                    or c.startswith('YRSIN')\n                                    or c.startswith('CITZ')\n                                    or c.startswith('AFVET_A')\n                                    or c.startswith('VADISB_A')\n                                    or c.startswith('HISP_A')\n                                    or c.startswith('HISDET')\n                                    or c.startswith('URB')\n                                    or c.startswith('INCGRP_A')\n                                    or c.startswith('RSNHI')\n                                    or c.startswith('NOTCOV')\n                                    ]\n\nnhis = nhis[columns]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"305b3a51-a1a3-4c02-9695-bd7eabcc3d00"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Cleaning BRFSS National Coverage Dataset**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae4c5919-1e71-4c6e-b90f-f075a1ba9fdf"}}},{"cell_type":"code","source":["MyAppToken = \"DSTTBNrVBNEeotyvAFx5bvhuT\"\nuser = \"ecarlson@dev-10.com\"\npassw = \"Hn3ft@fl\"\n\n# Example authenticated client (needed for non-public datasets):\nclient = Socrata(\"chronicdata.cdc.gov\",\n                  MyAppToken, username=user, password=passw)\n\n# Choos enumber of results, returned as JSON from API / converted to Python list of\n# dictionaries by sodapy.\napi_results = client.get(\"f7a2-7inb\",limit=158720)\n\n# Convert to pandas DataFrame\napi_results_df = pd.DataFrame.from_records(api_results)\n\n# filter data to only be from 2019\napi_results_df = api_results_df[api_results_df.year == '2019'].copy()\n\n# drop unnecessary columns\nbrfss_coverage_df = api_results_df.drop(columns = ['geolocation','breakoutid','breakoutcategoryid','topic','responseid','data_value_footnote','data_value_footnote_symbol',\n                'datasource','data_value_unit','data_value_unit','locationid','display_order',\n                'questionid','locationabbr','classid','data_value_type','topicid','class','confidence_limit_low','confidence_limit_high',\n                r':@computed_region_bxsw_vy29',r':@computed_region_he4y_prf8', 'year']).copy()\n\n# all null values are for data that does not exist, therefore null values are dropped.\nbrfss_coverage_df = brfss_coverage_df.dropna()\n\n# changing data types \nbrfss_coverage_df.data_value = brfss_coverage_df.data_value.astype(float)\nbrfss_coverage_df.sample_size = brfss_coverage_df.sample_size.astype(int)\n# brfss_coverage_df.info()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23e326f2-8b20-40b5-a0e5-47498bb8d5d2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Cleaning steps for S2703 and S2704**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c266833a-3fe8-4226-af7c-bd475a8aa48b"}}},{"cell_type":"code","source":["s2703 = spark.read.options(header = 'True').csv('/mnt/healthcare/dataIn/ACSST1Y2019.S2703-2022-09-21T120206.csv')\ns2704 = spark.read.options(header = 'True').csv('/mnt/healthcare/dataIn/ACSST1Y2019.S2704-2022-09-21T122429.csv')\n\ns2703 = s2703.toPandas()\ns2704 = s2704.toPandas()\n\n#Remove columns with total population numbers and perecentage numbers\ns2703 = s2703[s2703.columns.drop(list(s2703.filter(regex='Total')))]\ns2703 = s2703[s2703.columns.drop(list(s2703.filter(regex='Percent')))]\n\ns2704 = s2704[s2704.columns.drop(list(s2704.filter(regex='Total')))]\ns2704 = s2704[s2704.columns.drop(list(s2704.filter(regex='Percent')))]\n\n#remove data-lacking descriptive rows\ns2703 = s2703.drop(index = s2703.index[1])\ns2703 = s2703.drop(index = s2703.index[13])\ns2703= s2703.drop(index = s2703.index[25])\n\ns2704 = s2704.drop(index = s2704.index[1])\ns2704 = s2704.drop(index = s2704.index[13])\ns2704 = s2704.drop(index = s2704.index[25])\n\n# Renaming columns\ns2703 = s2703.rename(columns = { \\\n                                           s2703.columns[1]:'Anoka County', s2703.columns[2]:'Blue Earth County', \\\n                                           s2703.columns[3]:'Carver County', s2703.columns[4]:'Crow Wing County', \\\n                                           s2703.columns[5]:'Dakota County', s2703.columns[6]:'Hennepin County', \\\n                                           s2703.columns[7]:'Olmsted County', s2703.columns[8]:'Ramsey County', \\\n                                           s2703.columns[9]:'Rice County', s2703.columns[10]:'St. Louis County', \\\n                                           s2703.columns[11]:'Scott County', s2703.columns[12]:'Sherburne County', \\\n                                           s2703.columns[13]:'Stearns County', s2703.columns[14]:'Washington County', \\\n                                           s2703.columns[15]:'Wright County'}).copy()\n\ns2704 = s2704.rename(columns = { \\\n                                           s2704.columns[1]:'Anoka County', s2704.columns[2]:'Blue Earth County', \\\n                                           s2704.columns[3]:'Carver County', s2704.columns[4]:'Crow Wing County', \\\n                                           s2704.columns[5]:'Dakota County', s2704.columns[6]:'Hennepin County', \\\n                                           s2704.columns[7]:'Olmsted County', s2704.columns[8]:'Ramsey County', \\\n                                           s2704.columns[9]:'Rice County', s2704.columns[10]:'St. Louis County', \\\n                                           s2704.columns[11]:'Scott County', s2704.columns[12]:'Sherburne County', \\\n                                           s2704.columns[13]:'Stearns County', s2704.columns[14]:'Washington County', \\\n                                           s2704.columns[15]:'Wright County'}).copy()\n\n#transposing s2703 and s2704\ns2703_t = s2703.T\n\ns2704_t = s2704.T\n\n#resetting index for transposed dataset\n\nnew_header = s2703_t.iloc[0]\n\ns2703_t = s2703_t[1:]\n\ns2703_t.columns = new_header\n\ns2703_t = s2703_t.reset_index()\n\ns2703_t = s2703_t.rename(columns={'index':'County'})\n\n#dropping state-level row\nn = 1\n\ns2703_t.drop(s2703_t.tail(n).index,inplace=True)\n\n#resetting index for transposed dataset\nnew_header2 = s2704_t.iloc[0]\n\ns2704_t = s2704_t[1:]\n\ns2704_t.columns = new_header2\n\ns2704_t = s2704_t.reset_index()\n\ns2704_t = s2704_t.rename(columns={'index':'County'})\n\n#dropping state-level row\nn = 1\n\ns2704_t.drop(s2704_t.tail(n).index,inplace=True)\n\n#converting data types\ns2703_t = s2703_t.convert_dtypes()\n\ns2704_t = s2704_t.convert_dtypes()\n\n#drop Civilian noninstitutionalized population columns\ns2703_t = s2703_t.drop(['Civilian noninstitutionalized population'], axis = 1)\ns2704_t = s2704_t.drop(['Civilian noninstitutionalized population'], axis = 1)\n\n#remove commas in numerical columns\ns2703_t = s2703_t.replace(\",\",\"\", regex=True)\n\ns2704_t = s2704_t.replace(\",\",\"\", regex=True)\n\ns2703_t['State'] = 'Minnesota'\ns2704_t['State']= 'Minnesota'\n\n#setting column names for transfer back to spark dataframe\n\ns2703_t.columns = ['County', 'Employer', 'Employer<19', 'Employer19-64', 'Employer65+', 'Direct_Purchase', 'Direct_Purchase<19', \\\n                   'Direct_Purchase19-64', 'Direct_Purchase65+', 'Military', 'Military<19', 'Military19-64', 'Military65+', 'Below_138_Poverty', \\\n                   'AtorAbove_138_Poverty', 'Fulltime_Yearround', 'Fulltime<6Y', 'Fulltime6-18Y', 'Fulltime19-25Y', 'Fulltime26-34Y', 'Fulltime35-44Y', \\\n                   'Fulltime45-54Y', 'Fulltime55-64Y', 'Fulltime65-74Y', 'Fulltime75+', 'Private_Insurance_Alone', 'Employer_Alone', 'Direct_Purchase_Alone', \\\n                   'Military_Alone', 'State']\n\n#reordering placement of State column\ns2703_t = s2703_t[['County', 'State', 'Employer', 'Employer<19', 'Employer19-64', 'Employer65+', 'Direct_Purchase', 'Direct_Purchase<19',\\\n                   'Direct_Purchase19-64', 'Direct_Purchase65+', 'Military', 'Military<19', 'Military19-64', 'Military65+', 'Below_138_Poverty', \\\n                   'AtorAbove_138_Poverty', 'Fulltime_Yearround', 'Fulltime<6Y', 'Fulltime6-18Y', 'Fulltime19-25Y', 'Fulltime26-34Y', 'Fulltime35-44Y', \\\n                   'Fulltime45-54Y', 'Fulltime55-64Y', 'Fulltime65-74Y', 'Fulltime75+', 'Private_Insurance_Alone', 'Employer_Alone', 'Direct_Purchase_Alone', 'Military_Alone']]\n\n#setting column names for transfer back to spark dataframe\ns2704_t.columns = ['County', 'Medicare', 'Medicare<19', 'Medicare19-64', 'Medicare65+', 'Medicaid', 'Medicaid<19', \\\n                   'Medicaid19-64', 'Medicaid65+', 'VA', 'VA<19', 'VA19-64', 'VA65+', 'Below_138_Poverty', 'AtorAbove_138_Poverty', \\\n                   'Fulltime_Yearround', 'Fulltime<6Y', 'Fulltime6-18Y', 'Fulltime19-25Y', 'Fulltime26-34Y', 'Fulltime35-44Y', 'Fulltime45-54Y', \\\n                   'Fulltime55-64Y', 'Fulltime65-74Y', 'Fulltime75+', 'Public_Insurance_Alone', 'Medicare_Alone', 'Medicaid_Alone', 'VA_Alone', 'State']\n\n#reordering placement of State column\n\ns2704_t = s2704_t[['County', 'State', 'Medicare', 'Medicare<19', 'Medicare19-64', 'Medicare65+', 'Medicaid', 'Medicaid<19', \\\n                   'Medicaid19-64', 'Medicaid65+', 'VA', 'VA<19', 'VA19-64', 'VA65+', 'Below_138_Poverty', 'AtorAbove_138_Poverty', \\\n                   'Fulltime_Yearround', 'Fulltime<6Y', 'Fulltime6-18Y', 'Fulltime19-25Y', 'Fulltime26-34Y', 'Fulltime35-44Y', 'Fulltime45-54Y', \\\n                   'Fulltime55-64Y', 'Fulltime65-74Y', 'Fulltime75+', 'Public_Insurance_Alone', 'Medicare_Alone', 'Medicaid_Alone', 'VA_Alone']]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7859c79-5aab-40b0-8aef-ddf6124a70df"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Steps for cleaning BRF SMART datasets and creating sources for Question and Response tables**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d5101ee-b0e9-459d-a246-b62077b6eb23"}}},{"cell_type":"markdown","source":["**Reading in BRF Smart dataset and preparing it for further use**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7221dc31-53e0-4133-8a69-0c3159e8f664"}}},{"cell_type":"code","source":["smart2019 = spark.read.options(header = 'True').csv('/mnt/healthcare/dataIn/Behavioral_Risk_Factors__Selected_Metropolitan_Area_Risk_Trends__SMART__MMSA_Prevalence_Data__2011_to_Present_.csv')\n\nsmart2019 = smart2019.toPandas()\n\n#select area of interest in Class values\nsmart2019 = smart2019.loc[(smart2019['Class'] == 'Health Care Access/Coverage')]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9ac75ae-e53e-495c-b2d3-60e425ef0904"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Creating sources for Question and Response tables**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c2a419d-2dbe-4f77-8324-bd1fca25f2b6"}}},{"cell_type":"code","source":["smart2019_2 = spark.createDataFrame(smart2019)\n\nQ = smart2019_2.select('Question').distinct().sort('Question')\nR = smart2019_2.select('Response').distinct().sort('Response')\n\n\nQuestions = Q.toPandas()\nResponses = R.toPandas()\n\nindeces = []\nfor i in range(1, Questions.shape[0]+1):\n    indeces.append(i)\n\nQuestions['QuestionID'] = indeces\nQuestions.columns = ['Question', 'QuestionID']\nQuestions = Questions[['QuestionID', 'Question']]\n\nindeces = []\nfor i in range(1, Responses.shape[0]+1):\n    indeces.append(i)\n\nResponses['ResponseID'] = indeces\nResponses.columns = ['Response', 'ResponseID']\nResponses = Responses[['ResponseID', 'Response']]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e062a9fe-71fd-44ae-8137-678565210b45"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Cleaning BRFS Smart dataset**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fb1bd72-cc41-4530-84cd-059a22e006e8"}}},{"cell_type":"code","source":["#select desired columns\nsmart2019 = smart2019[['Locationdesc', 'Question', 'Response', 'Sample_Size', 'Data_value']]\n\n#change column data types\nsmart2019 = smart2019.convert_dtypes()\n\n#replace commas in dataframe\nsmart2019 = smart2019.replace(\",\",\"\", regex=True)\n\n#recast Sample_Size column as an int\nsmart2019['Sample_Size'] = smart2019['Sample_Size'].astype(int)\n\n#replace question/response with question/response ID\nsmart2019= smart2019.replace(['About how long has it been since you last visited a doctor for a routine checkup?', 'Adults aged 18-64 who have any kind of health care coverage (variable calculated from one or more BRFSS questions)', 'Do you have any kind of health care coverage?', 'Do you have one person you think of as your personal doctor or health care provider?', 'Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?'], [1,2,3,4,5])\n\nsmart2019=smart2019.replace(['5 or more years ago', 'More than one', 'Never', 'No', 'Within the past 2 years', 'Within the past 5 years', 'Within the past year', 'Yes', 'Yes only one'], [1,2,3,4,5,6,7,8,9])\n\n#renaming columns\nsmart2019.columns = ['Location_Desc', 'QuestionID', 'ResponseID', 'Sample_Size', 'Data_Value']\nsmart2019 = smart2019[['QuestionID', 'ResponseID', 'Location_Desc', 'Sample_Size', 'Data_Value']]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bf3e109-6943-45ca-9836-50087429bd61"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Cleaning Steps for sahie dataset**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2076288-d9cb-4c1c-8db3-fbdc281f0e94"}}},{"cell_type":"code","source":["n = 78\nsahie = spark.read.options(header = 'True').csv('/mnt/healthcare/dataIn/sahie_2019.csv')\\\n.rdd.zipWithIndex()\\\n.filter(lambda x: x[1] > n)\\\n.map(lambda x: x[0]).toDF()\n\nheader_list = ['year', 'version', 'StateID', 'countyfips', 'geocat', 'agecat', 'racecat', 'sexcat', 'incomecat', 'numdemo', 'numdemo_moe', 'NUI', 'nui_moe', 'NI', 'ni_moe', 'PCTUIdemo', 'PCTUIdemo_moe', 'PCTIdemo', 'PCTIdemo_moe', 'PCTUI', 'pctui_moe', 'PCTI', 'pcti_moe', 'state_name', 'county_name']\nnew_df = sahie.toDF(*header_list)\n\nsahie = new_df.toPandas()\n\nsahie.drop(columns=['version', 'StateID', 'countyfips', 'geocat'], inplace=True)\n\nsahie.drop(columns=['year'], inplace=True)\n\nsahie.replace('       .', np.NaN, inplace=True)\n\nsahie.replace('   . ', np.NaN, inplace=True)\n\nsahie['county_name'] = sahie['county_name'].str.replace(\"'\", \"\")\n\nfrom sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nsahie['numdemo'] = imp.fit_transform(sahie['numdemo'].values.reshape(-1,1))[:,0]\nsahie['numdemo_moe'] = imp.fit_transform(sahie['numdemo_moe'].values.reshape(-1,1))[:,0]\nsahie['NUI'] = imp.fit_transform(sahie['NUI'].values.reshape(-1,1))[:,0]\nsahie['nui_moe'] = imp.fit_transform(sahie['nui_moe'].values.reshape(-1,1))[:,0]\nsahie['PCTUIdemo'] = imp.fit_transform(sahie['PCTUIdemo'].values.reshape(-1,1))[:,0]\nsahie['PCTUIdemo_moe'] = imp.fit_transform(sahie['PCTUIdemo_moe'].values.reshape(-1,1))[:,0]\nsahie['NI'] = imp.fit_transform(sahie['NI'].values.reshape(-1,1))[:,0]\nsahie['ni_moe'] = imp.fit_transform(sahie['ni_moe'].values.reshape(-1,1))[:,0]\nsahie['PCTI'] = imp.fit_transform(sahie['PCTI'].values.reshape(-1,1))[:,0]\nsahie['pcti_moe'] = imp.fit_transform(sahie['pcti_moe'].values.reshape(-1,1))[:,0]\nsahie['PCTUI'] = imp.fit_transform(sahie['PCTUI'].values.reshape(-1,1))[:,0]\nsahie['pctui_moe'] = imp.fit_transform(sahie['pctui_moe'].values.reshape(-1,1))[:,0]\nsahie['PCTIdemo'] = imp.fit_transform(sahie['PCTIdemo'].values.reshape(-1,1))[:,0]\nsahie['PCTIdemo_moe'] = imp.fit_transform(sahie['PCTIdemo_moe'].values.reshape(-1,1))[:,0]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e947efbb-7a82-46c2-83a2-769fd171f547"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Cleaning Counties Data**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83ef689a-bccc-4723-aee2-512188cad999"}}},{"cell_type":"code","source":["location = spark.read.options(header = 'True').csv('/mnt/healthcare/dataIn/us_cities_states_counties.csv')\nlocation = location.toPandas()\n\nlocation[['City', 'State short', 'State full', 'County', 'City alias']] = location['City|State short|State full|County|City alias'].str.split('|', expand=True)\n\nlocation = location.drop(columns = ['City|State short|State full|County|City alias', 'City', 'State short', 'City alias', 'State full'])\nlocation = location.drop_duplicates()\n\nlocation = location.dropna()\n\nlocation[location['County'].isnull()]\nlocation = location.sort_values('County')\n\nfor i in location['County']:\n    if ' CITY' not in i:\n        location['County'].replace(i, (i.title() + ' County'), inplace = True)\n\nlocation['County'] = location['County'].str.title()\n\nlocation.drop(index = 8047, inplace = True)\n\nfor i in list(health['County'].unique()):\n    if i not in list(location['County']):\n        location.loc[len(location.index)] = i\n\nlocation = location.sort_values('County')\n\ncountyid = []\nfor i in range(1, len(location)+1):\n    countyid.append(i)\n    \nlocation['CountyID'] = countyid\n\nlocation = location[['CountyID', 'County']]\nlocation.columns = ['CountyID', 'CountyName']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"147a681f-3340-4266-a0ea-89c52989c72a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Creating State Table**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d461d05-95f1-433c-9822-f9ad6fea531e"}}},{"cell_type":"code","source":["State = pd.DataFrame(brfss_coverage_df['locationdesc'].str.replace(' ', '_').unique())\nState.replace('District_of_Columbia', 'DC', inplace = True)\n\nState2 = State.loc[[30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53]].copy()\nState.drop(index = [30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53], inplace = True)\nState.loc[len(State.index)] = 'New_Jersey'\n\nfor i in State2[0]:\n    State.loc[len(State.index)] = i\n\nrowIndex = []\nfor i in range(1, len(State)+1):\n    rowIndex.append(i)\nState['StateID'] = rowIndex\nState.rename(columns = {0:'StateName'}, inplace = True)\nState = State[['StateID', 'StateName']]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6135c84-c3da-48f5-a0ee-173c9a85f502"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Creating backups of cleaned dataframes**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99d2f3a6-caec-43fb-98f1-8b10d9e43a51"}}},{"cell_type":"code","source":["###### Mount Point 1 through Oauth security.\nstorageAccount = \"gen10datafund2207\"\nstorageContainer = \"healthcare-capstone-group3\"\nclientSecret = \"Cty8Q~AvEO_qC-MjvPvosYauiNsffOHKnMpj7cmd\"\nclientid = \"2ca50102-5717-4373-b796-39d06568588d\"\nmount_point = \"/mnt/healthcare/cleandataOut\" # the mount point will be unique to you\n\nconfigs = {\"fs.azure.account.auth.type\": \"OAuth\",\n       \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n       \"fs.azure.account.oauth2.client.id\": clientid,\n       \"fs.azure.account.oauth2.client.secret\": clientSecret,\n       \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/d46b54b2-a652-420b-aa5a-2ef7f8fc706e/oauth2/token\",\n       \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n\ntry: \n    dbutils.fs.unmount(mount_point)\nexcept:\n    pass\n\ndbutils.fs.mount(\nsource = \"abfss://\"+storageContainer+\"@\"+storageAccount+\".dfs.core.windows.net/\",\nmount_point = mount_point,\nextra_configs = configs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4801ebf8-59dc-40f1-93bf-f248f7044fec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/mnt/healthcare/cleandataOut has been unmounted.\nOut[6]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/mnt/healthcare/cleandataOut has been unmounted.\nOut[6]: True"]}}],"execution_count":0},{"cell_type":"code","source":["# Creating the spark dataframe\ndemoRename = spark.createDataFrame(demoRename)\nnewhospitals = spark.createDataFrame(newhospitals)\nhealth = spark.createDataFrame(health)\nnhis = spark.createDataFrame(nhis)\nbrfss_coverage_df = spark.createDataFrame(brfss_coverage_df)\nsahie = spark.createDataFrame(sahie)\ns2703_t = spark.createDataFrame(s2703_t)\ns2704_t = spark.createDataFrame(s2704_t)\nsmart2019 = spark.createDataFrame(smart2019)\nlocation = spark.createDataFrame(location)\nState = spark.createDataFrame(State)\nQuestions = spark.createDataFrame(Questions)\nResponses = spark.createDataFrame(Responses)\ntestdemo = spark.createDataFrame(testdemo)\n\n# Writing the cleaned dataframes to csv files\ndemoRename.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/Demographics.csv\", mode = 'overwrite')\nnewhospitals.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/HospitalCount.csv\", mode = 'overwrite')\nhealth.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/HealthInsuranceChar.csv\", mode = 'overwrite')\nnhis.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/NHIS.csv\", mode = 'overwrite')\nbrfss_coverage_df.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/BRFSSCoverage.csv\", mode = 'overwrite')\nsahie.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/sahie2019.csv\", mode = 'overwrite')\ns2703_t.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/CleanS2703MN.csv\", mode = 'overwrite')\ns2704_t.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/CleanS2704MN.csv\", mode = 'overwrite')\nsmart2019.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/CleanBRF_SMART_MN_Metros.csv\", mode = 'overwrite')\nlocation.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/County.csv\", mode = 'overwrite')\nState.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/State.csv\", mode = 'overwrite')\nQuestions.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/Question.csv\", mode = 'overwrite')\nResponses.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/Response.csv\", mode = 'overwrite')\ntestdemo.write.options(header = 'True').csv(\"/mnt/healthcare/cleandataOut/CleanedData/TestDemographics.csv\", mode = 'overwrite')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b62ee6c5-e535-4165-ab83-10e2dc3de358"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Dataset Cleaning","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3639924389947104}},"nbformat":4,"nbformat_minor":0}
